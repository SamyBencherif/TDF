<!DOCTYPE html>
<html>
<body>
<h2> Object Model Revision 2019-08-19 </h2>

<h3>Agenda</h3>

<p>
    <ol type="I">
	<li> Foreword </li>
	<li> Current Standard </li>
	<li> New Ideas &amp; Trade Study </li>
	<li> The Revised Standard </li>
	<li> Closing Remark &amp; Footnotes </li>
    </ol>
</p>

<h3> Foreword (I) </h3>

<p>
Presently, the object model consists of a memory layout, using C
structs, that is generated by a special function at runtime. This idea
seems redundant to me, in a sense, as the C language actually a very
special construct for describing executable regions of memory--C
functions, namely. However, it is important to consider that allowing
the object model specification to exist as a subset of C functions
would eschew reasonable hopes of creating source validation tools for
TD programs in the object model form. Obviously, parsing C-source or
Assembly code are NOT reasonable approaches. Validation must be done
strictly in advance of compilation. Thus implies the TD transpile step
must be proven to be semantically homomorphic.
</p>

<h3> Current Standard (II) </h3>

<p>
Before going into detail about the needs of the object model, it is
necessary to establish its purpose. One prime distinction between the
object model and the TD source file is the employment of a
<i>linearized instruction scheme</i>. That is, to take the 'no-loops,
no branches' paradigm a step further, the object model representation
will do away with direct use of even our own control structure: the
<i>invariant</i> (formerlly named ENSURE). Obviously, the
'no-branching' rule must be compromised, slightly (otherwise invariant
testing would be impossible). We will allow minimal branching, for the
event of system aborts only. By theoretical necessity, there will be
branching at some level of abstraction, should the system be able to
do one thing (proceed) or another (abort), so it is best to keep it
within the transpiled object model.
</p>

<h3> New Ideas &amp; Trade Study (III) </h3>

<p>
To minimize ambiguity, the object model will make use of only a
linearized C - function subset, cancelling some ideas from the previous
revision (2019-08-17, handwritten). This decision is the direct result
of the distinct characteristics of the two models (and their
incompatibilities). Consider the following pseudo code C samples.
</p>

<h4> generic model </h4>

<pre>
    void eval_prog(prog* p)
    {
        while(p-&gt;current_instruction != NULL)
        {
            result* res = doStep(p-&gt;current_instruction);
            if (res == failed)
            {
                return failed;
            }
            if (isInvariantLine(res))
            {
                stoInvariants(res);
            }
            advanceProgInstruction(p);
        }
    }
</pre>

<h4> explicit model </h4>

<pre>
    void run_prog_td_led()
    {
        td_check_invarients();
        td_set_light(OFF);
        td_check_invarients();
        td_wait(1000);
        td_check_invarients();
        td_wait(1000);
        td_set_light(ON);
        td_check_invarients();
        td_wait(1000);
        td_check_invarients();
        td_set_light(OFF);
        td_check_invarients();
    }
</pre>

<p>
This last example is what I was referring to when I said the code might
look repetitive to a person reading it. Expressing code in this form
reduces the need for control structures, thus reducing room for error.
In fact, it reduces the number of possible programs, just as intended.
Although the generic model, given ample adjustment, is far more elegant
than the explicit model, it would become the job of the runtime to
interpret how to deal with invariants, and that is NOT acceptable. Some
additional considerations about the models are as follows. The generic
model would appear to generate somewhat smaller intermediate source
programs and it is based in C structures, which allows the existence of
linear code validation tools without the need for further intermediate
stages. The explicit model allows for invariant testing to be defined
within the transpilation stage, however, it renders use of C structures
irrelevant.
</p>

<h3> The Revised Standard (IV) </h3>

<p>
Not all the aforementioned advantages and disadvantages bear the same
weight. The problem of invariant interpretation is significant enough
to yield the 'explicit model' as the more favorable choice. Conceptual
challenges sucg as <i>invariant testing latency<sup>1</sup></i>,
<i>adjusted time paritioning<sup>2</sup></i>, and
<i>invariant subcategories<sup>3</sup></i> are subject,
perhaps, to a need for tuning and would, under a generic model, induce
non-linear structures. Something along the lines of storage and referal
(in which case state-fullness would be the culprit) or inclusion of
seperate cases for instructions and invariant testing within a loop
that traverses a linked list. Problems with the latter include a
resistence to time partitioning. As a result, the explicit model is
adopted, and the viability for object model validation is lost. To make
up for this: (1) an exceptionally rigorous transpilation step is needed
and (2) TD language validation shall be devised--whether or not with
the inclusion of a new linearized, pre-object model intermediary is
not known at this time (see timestamp).
</p>

<p>
TD program validation is a critical step that does not ship with the
initial release fo the TD transpiler. It is recommended not to run
any testbed programs until a validation suite is also released.
Engineers are welcome to examine, verify, and implement hardware level
code in the meantime. The initial release is intended for those
purposes.
</p>

<h3> Closing Remark &amp; Footnotes (V) </h3>

<p>
The new object model standard calls for the disbanding of C structure
modeled code. All other spec details from the manual should be adhered
to as closely as possible.
</p>

<h4>1: Invariant Testing Latency</h4>

<p>
TD programs are extensively time sensitive. If we run (TD source,
pseudo):
</p>

<pre>
    set valve open
    wait 3000 ms
    set valve close
</pre>

<p>
We wish for the pause between those statements to be very close to
3000 ms. Compromise settles in, in service of invariant testing. Like
any computational effort, invariant testing takes some time. So
<i>adjusted time partitioning</i> may be desireable.
</p>

<h4>2: Adjusted Time Partitioning</h4>

<p>
First, to explain time paritioning in general, when we perform a 'wait'
of a specific duration, we might be interested in invariant test
results <i>during</i> that 'wait'. In fact, we are interested. We can
keep track of that information using time partitioning. Demonstrated
as so:
</p>

<pre>
     |___wait 1000 ms___| |___wait 1000 ms___| |___wait 1000 ms___|
                         ^                    ^
                         [check invariants]   [check invariants]
</pre>

<p>
Adjusted time partitioning is the practice of preforming the
first partition of, say a third the wait time, measuring the total
time of the first third plus the amount of time it takes to
preform the first invariant test, then taking the total desired
wait time minus this elapsed time, and now splitting it in half.
The process caries on in this manner. So, to wait 3000 ms:
</p>

<pre>
     |___wait 1000 ms___| &lt; [check elapsed] ...
    ^                    ^
    [start timer]        [check invariants]
</pre>

<p>
Say that took 1045 ms to complete. Now we have (3000-1045) = 1955 ms
left to wait. We then divide this job of waiting into two pieces.
Now (1955/2) = 977.5 ms of waiting remains. Here I show our timed
totals, starring the steps whose duration we do not control:
</p>

<pre>
    [wait: 1000 ms][*check: 45 ms][wait: 977.5 ms][*check: 16ms]
</pre>

<p>
And so the final wait time can be calculated to be (3000 - 2038.5) =
961.5 ms. As you can see, We can correct completely for <i>invariant
testing latency</i> as long as a final check at the end of the wait
can be forgone and the number of time partitions is not overly
ambitious. (Time Total / Partition Count should be &gt;&gt; max
invariant test latency.) Thanks to the inclusion of <i>Invariant
Subcategories</i> the TD programmer can have minimal influence of when
these tests are preformed.
</p>

<h4>3: Invariant Subcategories</h4>

<p>
This term refers to the inclusion of specific invariant types
such as 'before', 'during', and 'after', which specify invariant
test occurence relative to an 'invariant' block in a TD program. For
example (TD pseudocode):
</p>

<pre>
    invariant
    before: during:
    moon_radius &gt; 500 m, earth_mass &lt; 1e9 kg,
    0 &lt; time, time &lt; 3000 ms;
    after:;
    {
        set valve open
        wait 3000 ms
    }
    invariant
    before: during:;
    after:
    time &lt; 3500 ms;
    {
        set valve closed;
    }

</pre>

<p>
This program makes use of invariant subcategory keywords to specify
that no invariant testing should occur between blocks 1 and 2. This
strategy is to be employed when precision in wait time is important
enough to forgo a system integrity check (invariant test). Try to use
this method sparingly.
</p>

<h4> Closing  </h4>

<p>
Because this revision eliminates the abstract memory model formerly
used in favor of a more 'linear' model. It is no longer possible
(reasonable) to run validation tests on compiled TD programs. As a
result it is worth considering introducing a linearized intermediary
format intended for use with code testing software. Alternatively, (or
additionally) it is possible to profile using dummy implementations of
hardware interface code. These next considerations will come into play
during the conception of the TD language standard and TD compiler.
</p>

</body>
</html>
